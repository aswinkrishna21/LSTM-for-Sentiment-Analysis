{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":67,"outputs":[{"output_type":"stream","text":"/kaggle/input/twitter-sentiment-analysis/test.csv\n/kaggle/input/twitter-sentiment-analysis/train.csv\n/kaggle/input/twitter-sentiment-analysis/sample_submission_gfvA5FD.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Loading the Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/twitter-sentiment-analysis/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/twitter-sentiment-analysis/test.csv\")","execution_count":68,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Text Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":69,"outputs":[{"output_type":"execute_result","execution_count":69,"data":{"text/plain":"   id  label                                              tweet\n0   1      0   @user when a father is dysfunctional and is s...\n1   2      0  @user @user thanks for #lyft credit i can't us...\n2   3      0                                bihday your majesty\n3   4      0  #model   i love u take with u all the time in ...\n4   5      0             factsguide: society now    #motivation","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>label</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>@user when a father is dysfunctional and is s...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n      <td>@user @user thanks for #lyft credit i can't us...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0</td>\n      <td>bihday your majesty</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>#model   i love u take with u all the time in ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>factsguide: society now    #motivation</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets = train['tweet']\nlabels = train['label']","execution_count":70,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nimport nltk","execution_count":71,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from string import punctuation\nfrom nltk.corpus import stopwords","execution_count":72,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"appos = {\"aren't\" : \"are not\", \"can't\" : \"cannot\", \"couldn't\" : \"could not\", \"didn't\" : \"did not\", \"doesn't\" : \"does not\",\n\"don't\" : \"do not\", \"hadn't\" : \"had not\", \"hasn't\" : \"has not\", \"haven't\" : \"have not\", \"he'd\" : \"he would\",\n\"he'll\" : \"he will\", \"he's\" : \"he is\", \"i'd\" : \"I would\", \"i'd\" : \"I had\", \"i'll\" : \"I will\", \"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\"it's\" : \"it is\", \"it'll\":\"it will\", \"i've\" : \"I have\", \"let's\" : \"let us\", \"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\", \"shan't\" : \"shall not\", \"she'd\" : \"she would\", \"she'll\" : \"she will\", \"she's\" : \"she is\",\n\"shouldn't\" : \"should not\", \"that's\" : \"that is\", \"there's\" : \"there is\", \"they'd\" : \"they would\", \"they'll\" : \"they will\",\n\"they're\" : \"they are\", \"they've\" : \"they have\", \"we'd\" : \"we would\", \"we're\" : \"we are\", \"weren't\" : \"were not\",\n\"we've\" : \"we have\", \"what'll\" : \"what will\",\"what're\" : \"what are\",\"what's\" : \"what is\", \"what've\" : \"what have\",\n\"where's\" : \"where is\", \"who'd\" : \"who would\", \"who'll\" : \"who will\", \"who're\" : \"who are\", \"who's\" : \"who is\",\n\"who've\" : \"who have\", \"won't\" : \"will not\", \"wouldn't\" : \"would not\", \"you'd\" : \"you would\", \"you'll\" : \"you will\",\n\"you're\" : \"you are\", \"you've\" : \"you have\", \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\":\" will\", \"didn't\": \"did not\"}","execution_count":73,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(text):\n    all_tweets = list()\n    for txt in text:\n        lower_case = txt.lower()\n        words = lower_case.split()\n        formatted = [appos[word] if word in appos else word for word in words]\n        formatted_test = list()\n        for word in formatted:\n            if word not in stopwords.words(\"english\"):\n                formatted_test.append(word)\n        formatted = \" \".join(formatted_test)\n        punct_text = \"\".join([ch for ch in formatted if ch not in punctuation])\n        all_tweets.append(punct_text)\n    for i in range(len(all_tweets)):\n        if all_tweets[i].startswith(\"user\"):\n            all_tweets[i] = all_tweets[i].replace(\"user\", '')\n    all_text = \" \".join(all_tweets)\n    all_words = all_text.split()\n    \n    return all_tweets, all_words","execution_count":74,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_tweets, all_words = preprocess(tweets)","execution_count":75,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating Dictionaries and encoding the words"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_tweets[3]","execution_count":76,"outputs":[{"output_type":"execute_result","execution_count":76,"data":{"text/plain":"'model love u take u time urð\\x9f\\x93± ð\\x9f\\x98\\x99ð\\x9f\\x98\\x8eð\\x9f\\x91\\x84ð\\x9f\\x91 ð\\x9f\\x92¦ð\\x9f\\x92¦ð\\x9f\\x92¦'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nfor i in range(len(all_tweets)):\n    all_tweets[i] = re.sub('[^a-zA-Z0-9]', ' ', all_tweets[i])\n\nall_words = []\nfor sentence in all_tweets:\n    for word in sentence.split():\n        all_words.append(word)","execution_count":77,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\nword_counts = Counter(all_words)\nword_list = sorted(word_counts, reverse = True)\nword2int = {word : i+1 for (i, word) in enumerate(word_list)}\nint2word = {i : word for word, i in word2int.items()}\n\nencoded_tweets = [[word2int[word] for word in tweet.split()] for tweet in all_tweets]","execution_count":78,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Encoding labels is not required as they themselves are binary (1 or 0)"},{"metadata":{},"cell_type":"markdown","source":"Getting rid of short reviews"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_labels = np.array([label for idx, label in enumerate(labels) if len(encoded_tweets[idx]) > 0])\nencoded_tweets = [tweet for tweet in encoded_tweets if len(tweet) > 0]","execution_count":79,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Padding the sequences (making the length of all tweets the same)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def pad_tweet(encoded_tweets, tweet_length):\n    Tweets = []\n    \n    for tweet in encoded_tweets:\n        if len(tweet) >= tweet_length:\n            Tweets.append(tweet[:tweet_length])\n        else:\n            Tweets.append([0] * (tweet_length - len(tweet)) + tweet)\n    return np.array(Tweets)","execution_count":80,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"padded_reviews = pad_tweet(encoded_tweets, 15)","execution_count":81,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split Data into train and validation sets and get (tweet, label) dataloaders"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn","execution_count":82,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ratio = 0.9\nvalid_ratio = 0.1\ntotal = padded_reviews.shape[0]\ntrain_cutoff = int(total * train_ratio)\n\nx_train, y_train = padded_reviews[:train_cutoff], encoded_labels[:train_cutoff]\nx_valid, y_valid = padded_reviews[train_cutoff:], encoded_labels[train_cutoff:]\n\n\nfrom torch.utils.data import TensorDataset, DataLoader\n\ntrain_data = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\nvalid_data = TensorDataset(torch.from_numpy(x_valid), torch.from_numpy(y_valid))\n\nbatch_size = 32\n\ntrain_loader = DataLoader(train_data, batch_size, shuffle = True, drop_last = True)\nvalid_loader = DataLoader(valid_data, batch_size, shuffle = True, drop_last = True)","execution_count":83,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defining the LSTM Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class sentimentLSTM(nn.Module):\n    def __init__(self, input_size, embedding_dim, hidden_size, output_size, n_layers, drop_prob = 0.3):\n        super().__init__()\n        self.input_size = input_size\n        self.embedding_dim = embedding_dim\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n        \n        self.embed = nn.Embedding(input_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_size, n_layers, batch_first = True, dropout = drop_prob)\n        self.dropout = nn.Dropout(drop_prob)\n        self.fc = nn.Linear(hidden_size, output_size)\n        self.sig = nn.Sigmoid()\n        \n    def forward(self, input_words, h):\n        #Input dimension = batch_size x tweet_length\n        batch_size = input_words.shape[0]\n        embedd = self.embed(input_words) #dimension = batch_size x tweet_length x embedding_dim\n        \n        lstm_out, h = self.lstm(embedd, h) #dimension = batch_size x tweet_length x hidden_size\n        lstm_out = self.dropout(lstm_out)\n        #stacking up the lstm outputs\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_size) #dimension = (batch_size * tweet_length) x hidden_size\n        \n        fc_out = self.fc(lstm_out) #dimension = (batch_size * tweet_length) x output_size\n        \n        sig_out = self.sig(fc_out) #dimension = (batch_size * tweet_length) x output_size\n        sig_out = sig_out.view(batch_size, -1) #dimension = batch_size x (tweet_length * output_size)\n        sig_out = sig_out[:, -1] #Extract only the last output of the element of each example in the batch\n        \n        return sig_out, h\n    \n    def init_hidden(self, batch_size):\n        weights = next(self.parameters()).data\n        \n        h = (weights.new(self.n_layers, batch_size, self.hidden_size).zero_(),\n             weights.new(self.n_layers, batch_size, self.hidden_size).zero_())\n        \n        return h","execution_count":129,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Initialize the hyperparameters of the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_size = len(word2int)+1\nembedding_dim = 400\nhidden_size = 256\noutput_size = 1\nn_layers = 2","execution_count":130,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = sentimentLSTM(input_size, embedding_dim, hidden_size, output_size, n_layers)","execution_count":131,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model","execution_count":132,"outputs":[{"output_type":"execute_result","execution_count":132,"data":{"text/plain":"sentimentLSTM(\n  (embed): Embedding(41321, 400)\n  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.3)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc): Linear(in_features=256, out_features=1, bias=True)\n  (sig): Sigmoid()\n)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\ncriterion = nn.BCELoss()","execution_count":133,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_every = 100\nstep = 0\nn_epochs = 5\nclip = 5\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ntrain_losses = []\n\nif torch.cuda.is_available():\n    model.cuda()\n\nmodel.train()\nfor epoch in range(1, n_epochs+1):\n    h = model.init_hidden(batch_size)\n    \n    for inputs, labels in train_loader:\n        step += 1\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        h = tuple([each.data for each in h]) \n        \n        model.zero_grad()\n        output, h = model(inputs, h)\n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        \n        if step % print_every == 0:\n            #Validation\n            valid_losses = []\n            valid_h = model.init_hidden(batch_size)\n            model.eval()\n            \n            for valid_inputs, valid_labels in valid_loader:\n                valid_inputs, valid_labels = valid_inputs.to(device), valid_labels.to(device)\n                \n                valid_h = tuple([each.data for each in valid_h])\n                \n                valid_output, valid_h = model(valid_inputs, valid_h)\n                valid_loss = criterion(valid_output.squeeze(), valid_labels.float())\n                valid_losses.append(valid_loss.item())\n                train_losses.append(loss.item())\n                \n            print(\"Epoch: {}/{}\".format((epoch), n_epochs),\n                  \"Step: {}\".format(step),\n                  \"Training Loss: {:.4f}\".format(loss.item()),\n                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n            model.train()","execution_count":134,"outputs":[{"output_type":"stream","text":"Epoch: 1/5 Step: 100 Training Loss: 0.3335 Validation Loss: 0.2508\nEpoch: 1/5 Step: 200 Training Loss: 0.0622 Validation Loss: 0.2177\nEpoch: 1/5 Step: 300 Training Loss: 0.4258 Validation Loss: 0.2137\nEpoch: 1/5 Step: 400 Training Loss: 0.4704 Validation Loss: 0.1749\nEpoch: 1/5 Step: 500 Training Loss: 0.0744 Validation Loss: 0.1499\nEpoch: 1/5 Step: 600 Training Loss: 0.0699 Validation Loss: 0.1463\nEpoch: 1/5 Step: 700 Training Loss: 0.0709 Validation Loss: 0.1407\nEpoch: 1/5 Step: 800 Training Loss: 0.0757 Validation Loss: 0.1476\nEpoch: 2/5 Step: 900 Training Loss: 0.0259 Validation Loss: 0.1362\nEpoch: 2/5 Step: 1000 Training Loss: 0.2857 Validation Loss: 0.1355\nEpoch: 2/5 Step: 1100 Training Loss: 0.0925 Validation Loss: 0.1639\nEpoch: 2/5 Step: 1200 Training Loss: 0.1203 Validation Loss: 0.1441\nEpoch: 2/5 Step: 1300 Training Loss: 0.0050 Validation Loss: 0.1543\nEpoch: 2/5 Step: 1400 Training Loss: 0.0144 Validation Loss: 0.1459\nEpoch: 2/5 Step: 1500 Training Loss: 0.0020 Validation Loss: 0.1537\nEpoch: 2/5 Step: 1600 Training Loss: 0.0691 Validation Loss: 0.1394\nEpoch: 2/5 Step: 1700 Training Loss: 0.0191 Validation Loss: 0.1322\nEpoch: 3/5 Step: 1800 Training Loss: 0.0099 Validation Loss: 0.1455\nEpoch: 3/5 Step: 1900 Training Loss: 0.1811 Validation Loss: 0.1666\nEpoch: 3/5 Step: 2000 Training Loss: 0.0783 Validation Loss: 0.2175\nEpoch: 3/5 Step: 2100 Training Loss: 0.1757 Validation Loss: 0.1979\nEpoch: 3/5 Step: 2200 Training Loss: 0.0151 Validation Loss: 0.1659\nEpoch: 3/5 Step: 2300 Training Loss: 0.0081 Validation Loss: 0.1882\nEpoch: 3/5 Step: 2400 Training Loss: 0.0714 Validation Loss: 0.1884\nEpoch: 3/5 Step: 2500 Training Loss: 0.0389 Validation Loss: 0.1862\nEpoch: 3/5 Step: 2600 Training Loss: 0.0033 Validation Loss: 0.1741\nEpoch: 4/5 Step: 2700 Training Loss: 0.0176 Validation Loss: 0.1910\nEpoch: 4/5 Step: 2800 Training Loss: 0.0010 Validation Loss: 0.1850\nEpoch: 4/5 Step: 2900 Training Loss: 0.0057 Validation Loss: 0.2071\nEpoch: 4/5 Step: 3000 Training Loss: 0.0421 Validation Loss: 0.2025\nEpoch: 4/5 Step: 3100 Training Loss: 0.0036 Validation Loss: 0.1901\nEpoch: 4/5 Step: 3200 Training Loss: 0.0387 Validation Loss: 0.1795\nEpoch: 4/5 Step: 3300 Training Loss: 0.0044 Validation Loss: 0.2158\nEpoch: 4/5 Step: 3400 Training Loss: 0.0036 Validation Loss: 0.1893\nEpoch: 4/5 Step: 3500 Training Loss: 0.0040 Validation Loss: 0.1643\nEpoch: 5/5 Step: 3600 Training Loss: 0.0141 Validation Loss: 0.2199\nEpoch: 5/5 Step: 3700 Training Loss: 0.0509 Validation Loss: 0.1890\nEpoch: 5/5 Step: 3800 Training Loss: 0.0029 Validation Loss: 0.1993\nEpoch: 5/5 Step: 3900 Training Loss: 0.0010 Validation Loss: 0.2399\nEpoch: 5/5 Step: 4000 Training Loss: 0.0345 Validation Loss: 0.2012\nEpoch: 5/5 Step: 4100 Training Loss: 0.0074 Validation Loss: 0.2212\nEpoch: 5/5 Step: 4200 Training Loss: 0.1391 Validation Loss: 0.1959\nEpoch: 5/5 Step: 4300 Training Loss: 0.0035 Validation Loss: 0.2233\nEpoch: 5/5 Step: 4400 Training Loss: 0.0197 Validation Loss: 0.1929\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_tweets = test['tweet']\ntest_tweets, test_words = preprocess(test_tweets)","execution_count":135,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(test_tweets)):\n    test_tweets[i] = re.sub('[^a-zA-Z0-9]', ' ', test_tweets[i])\n\ntest_words = []\nfor sentence in test_tweets:\n    for word in sentence.split():\n        test_words.append(word)","execution_count":136,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_test_tweets = []\nfor tweet in test_tweets:\n    encoded_tweet = []\n    for word in tweet.split():\n        if word not in word2int.keys():\n            encoded_tweet.append(0)\n        else:\n            encoded_tweet.append(word2int[word])\n    encoded_test_tweets.append(encoded_tweet)","execution_count":137,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"padded_test_tweets = pad_tweet(encoded_test_tweets, 15)","execution_count":138,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_model(test_input):\n    output_list = list()\n    model.eval()\n    with torch.no_grad():\n        for tweet in test_input:\n            feature_tensor = torch.from_numpy(tweet).view(1, -1)\n            if(torch.cuda.is_available()):\n                feature_tensor = feature_tensor.cuda()\n            batch_size = feature_tensor.size(0)\n            #initialize hidden state\n            h = model.init_hidden(batch_size)\n            #get the output from the model\n            output, h = model(feature_tensor, h)\n            pred = torch.round(output.squeeze())\n            output_list.append(pred)\n        test_labels = [int(i.data.cpu().numpy()) for i in output_list]\n        return test_labels","execution_count":139,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labels = test_model(padded_test_tweets)","execution_count":140,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame()\noutput['id'] = test['id']\noutput['label'] = test_labels","execution_count":145,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output","execution_count":146,"outputs":[{"output_type":"execute_result","execution_count":146,"data":{"text/plain":"          id  label\n0      31963      0\n1      31964      0\n2      31965      0\n3      31966      0\n4      31967      0\n...      ...    ...\n17192  49155      1\n17193  49156      0\n17194  49157      0\n17195  49158      0\n17196  49159      0\n\n[17197 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>31963</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>31964</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>31965</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>31966</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>31967</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>17192</th>\n      <td>49155</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17193</th>\n      <td>49156</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17194</th>\n      <td>49157</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17195</th>\n      <td>49158</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17196</th>\n      <td>49159</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>17197 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"output.to_csv(\"subm.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}